{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "# Download necessary NLTK data\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    nltk.data.find('corpora/omw-1.4')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751e0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "BASE_PATH = Path('../../data/social-media-extremism-detection-challenge')\n",
    "ori_train_df = pd.read_csv(BASE_PATH / 'train.csv')\n",
    "BASE_LLM_PATH = BASE_PATH / 'results'\n",
    "train_df = pd.read_csv(BASE_LLM_PATH / 'llm_oof_train.csv')\n",
    "test_df = pd.read_csv(BASE_LLM_PATH / 'llm_test_preds.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d21b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting root words and merging derivatives...\n",
      "                             Original_Message  \\\n",
      "0       sixth forms should burn to the ground   \n",
      "1  whatever should burn benders to the ground   \n",
      "2         factories should burn to the ground   \n",
      "3   halal factories should burn to the ground   \n",
      "4               nhs should burn to the ground   \n",
      "\n",
      "                       unique_roots  \n",
      "0        [grind, form, sixth, burn]  \n",
      "1  [benders, whatever, burn, grind]  \n",
      "2          [grind, burn, factories]  \n",
      "3   [halal, grind, burn, factories]  \n",
      "4                [nhs, burn, grind]  \n"
     ]
    }
   ],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Default to Noun\n",
    "def extract_root_words(text):\n",
    "    # 1. Cleaning\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    \n",
    "    # 2. Tokenize\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 3. Filter Stopwords & Lemmatize (Merge derivatives)\n",
    "    # We use a set() to ensure we only count UNIQUE words per comment (Step 1)\n",
    "    root_words = set()\n",
    "    for word in tokens:\n",
    "        if word not in stop_words and len(word) > 2:\n",
    "            # Step 4 happens here: 'killing' becomes 'kill'\n",
    "            root = lemmatizer.lemmatize(word, pos='v') # 'v' focuses on verbs (actions)\n",
    "            root_words.add(root)\n",
    "            \n",
    "    return list(root_words)\n",
    "def extract_root_words_smart(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Tag the words (Is it a noun? A verb?)\n",
    "    # returns list like [('kill', 'VB'), ('traitors', 'NNS')]\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    unique_roots = set()\n",
    "    \n",
    "    for word, tag in tagged_tokens:\n",
    "        if word not in stop_words and len(word) > 2:\n",
    "            # Get the correct POS type (Noun/Verb/Adj)\n",
    "            wntag = get_wordnet_pos(tag)\n",
    "            \n",
    "            # Lemmatize with the SPECIFIC tag\n",
    "            # 'traitors' + NOUN -> 'traitor'\n",
    "            # 'killing' + VERB -> 'kill'\n",
    "            # 'worse' + ADJ -> 'bad'\n",
    "            root = lemmatizer.lemmatize(word, pos=wntag)\n",
    "            unique_roots.add(root)\n",
    "            \n",
    "    return list(unique_roots)\n",
    "\n",
    "print(\"Extracting root words and merging derivatives...\")\n",
    "# Apply to your dataframe\n",
    "train_df['unique_roots'] = train_df['Original_Message'].apply(extract_root_words)\n",
    "test_df['unique_roots'] = test_df['Original_Message'].apply(extract_root_words)\n",
    "print(train_df[['Original_Message', 'unique_roots']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d00662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Safe Docs: 1101\n",
      "Total Extremist Docs: 1149\n"
     ]
    }
   ],
   "source": [
    "# 1. Explode the list column\n",
    "# This creates a new row for every single word in the list, preserving the label.\n",
    "# It makes counting incredibly easy using pandas groupby.\n",
    "exploded_df = train_df.explode('unique_roots')\n",
    "\n",
    "# 2. Calculate Frequencies (Step 3)\n",
    "# We group by Label (target) and Word (unique_roots) and count the rows\n",
    "word_counts = exploded_df.groupby(['target', 'unique_roots']).size().reset_index(name='doc_count')\n",
    "\n",
    "# 3. Pivot to compare Extremist vs Safe side-by-side\n",
    "lexicon = word_counts.pivot(index='unique_roots', columns='target', values='doc_count').fillna(0)\n",
    "lexicon.columns = ['Safe_Count', 'Extremist_Count']\n",
    "\n",
    "# 4. Add Total Counts for Context\n",
    "total_safe_docs = train_df['target'].value_counts()[0]\n",
    "total_extremist_docs = train_df['target'].value_counts()[1]\n",
    "\n",
    "print(f\"Total Safe Docs: {total_safe_docs}\")\n",
    "print(f\"Total Extremist Docs: {total_extremist_docs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3614d0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top Derived Roots for Extremism ---\n",
      "               Extremist_Count  Safe_Count  Extremism_Ratio\n",
      "unique_roots                                               \n",
      "anymore                   25.0         0.0     21758.050479\n",
      "encourage                 20.0         0.0     17406.440383\n",
      "purpose                   20.0         0.0     17406.440383\n",
      "value                     19.0         0.0     16536.118364\n",
      "adhere                    17.0         0.0     14795.474326\n",
      "applaud                   14.0         0.0     12184.508268\n",
      "rapists                   12.0         0.0     10443.864230\n",
      "computer                   9.0         0.0      7832.898172\n",
      "serve                      9.0         0.0      7832.898172\n",
      "allah                      9.0         0.0      7832.898172\n",
      "asians                     9.0         0.0      7832.898172\n",
      "ugh                        8.0         0.0      6962.576153\n",
      "thieve                     8.0         0.0      6962.576153\n",
      "murderers                  8.0         0.0      6962.576153\n",
      "niggers                    8.0         0.0      6962.576153\n",
      "worshipers                 7.0         0.0      6092.254134\n",
      "deportthemall              7.0         0.0      6092.254134\n",
      "cheer                      7.0         0.0      6092.254134\n",
      "religious                  7.0         0.0      6092.254134\n",
      "forth                      7.0         0.0      6092.254134\n"
     ]
    }
   ],
   "source": [
    "# Calculate P(Word | Class)\n",
    "# \"What % of extremist comments contain this word?\"\n",
    "lexicon['Safe_Prob'] = lexicon['Safe_Count'] / total_safe_docs\n",
    "lexicon['Extremist_Prob'] = lexicon['Extremist_Count'] / total_extremist_docs\n",
    "\n",
    "# Calculate the \"Extremism Score\" (Ratio)\n",
    "# We add a tiny epsilon (1e-6) to avoid dividing by zero\n",
    "lexicon['Extremism_Ratio'] = lexicon['Extremist_Prob'] / (lexicon['Safe_Prob'] + 1e-6)\n",
    "\n",
    "# Filter for noise (Words that appear in at least 5 extremist docs)\n",
    "robust_lexicon = lexicon[lexicon['Extremist_Count'] >= 5].copy()\n",
    "\n",
    "# Sort by the most \"Distinctively Extremist\" words\n",
    "top_extremist_words = robust_lexicon.sort_values(by='Extremism_Ratio', ascending=False)\n",
    "\n",
    "print(\"--- Top Derived Roots for Extremism ---\")\n",
    "print(top_extremist_words[['Extremist_Count', 'Safe_Count', 'Extremism_Ratio']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8022140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 'lexicon_weights.json'\n"
     ]
    }
   ],
   "source": [
    "# Convert lexicon to a dictionary for fast mapping\n",
    "ratio_dict = top_extremist_words['Extremism_Ratio'].to_dict()\n",
    "import json\n",
    "with open('lexicon_weights.json', 'w') as f:\n",
    "    json.dump(ratio_dict, f)\n",
    "    \n",
    "print(\"Saved 'lexicon_weights.json'\")\n",
    "def get_lexicon_score(root_list):\n",
    "    score = 0\n",
    "    for word in root_list:\n",
    "        if word in ratio_dict:\n",
    "            score += ratio_dict[word]\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d0c3b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Extremist Words (Highest Log-Odds):\n",
      "              Safe_Count  Extremist_Count  log_odds\n",
      "unique_roots                                       \n",
      "anymore              0.0             25.0  3.215461\n",
      "purpose              0.0             20.0  3.001887\n",
      "encourage            0.0             20.0  3.001887\n",
      "value                0.0             19.0  2.953097\n",
      "adhere               0.0             17.0  2.847737\n",
      "applaud              0.0             14.0  2.665415\n",
      "rapists              0.0             12.0  2.522314\n",
      "serve                0.0              9.0  2.259950\n",
      "computer             0.0              9.0  2.259950\n",
      "allah                0.0              9.0  2.259950\n"
     ]
    }
   ],
   "source": [
    "lexicon = word_counts.pivot(index='unique_roots', columns='target', values='doc_count').fillna(0)\n",
    "\n",
    "# Rename columns (0 -> Safe_Count, 1 -> Extremist_Count)\n",
    "lexicon.columns = ['Safe_Count', 'Extremist_Count']\n",
    "\n",
    "# 2. CALCULATE TOTALS (for normalization)\n",
    "# We need to know the total number of docs in each class to calculate probabilities\n",
    "total_safe_docs = train_df['target'].value_counts()[0]\n",
    "total_ext_docs = train_df['target'].value_counts()[1]\n",
    "\n",
    "# 3. CALCULATE LOG-ODDS (With Laplace Smoothing +1)\n",
    "# Smoothing prevents division by zero and infinite logs\n",
    "\n",
    "# Probability of word given Extremist\n",
    "p_w_ext = (lexicon['Extremist_Count'] + 1) / (total_ext_docs + 1)\n",
    "\n",
    "# Probability of word given Safe\n",
    "p_w_safe = (lexicon['Safe_Count'] + 1) / (total_safe_docs + 1)\n",
    "\n",
    "# Log-Odds Ratio\n",
    "# Positive Value (>0) = Strongly Associated with Extremism\n",
    "# Negative Value (<0) = Strongly Associated with Safety\n",
    "# Value near 0 = Neutral / Common Word\n",
    "lexicon['log_odds'] = np.log(p_w_ext / p_w_safe)\n",
    "\n",
    "# Convert to dictionary for fast lookup\n",
    "log_odds_dict = lexicon['log_odds'].to_dict()\n",
    "\n",
    "print(\"Top Extremist Words (Highest Log-Odds):\")\n",
    "print(lexicon.sort_values(by='log_odds', ascending=False).head(10))\n",
    "def get_max_log_odds(root_list):\n",
    "    if not isinstance(root_list, list) or len(root_list) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    scores = []\n",
    "    for word in root_list:\n",
    "        if word in log_odds_dict:\n",
    "            # We only care about Positive scores (Words indicating Extremism)\n",
    "            # If a word is \"Safe\" (Negative score), we ignore it.\n",
    "            val = log_odds_dict[word]\n",
    "            if val > 0:\n",
    "                scores.append(val)\n",
    "    \n",
    "    if not scores:\n",
    "        return 0.0\n",
    "    \n",
    "    # Return the Single Highest Extremism Score found in the sentence\n",
    "    return max(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad934633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe which has only index and lexicon score\n",
    "Lexicon_Score_train = train_df[['ID']].copy()\n",
    "Lexicon_Score_train['lexicon_score'] = train_df['unique_roots'].apply(get_lexicon_score)\n",
    "Lexicon_Score_train['lexicon_score_log'] = train_df['unique_roots'].apply(get_max_log_odds)\n",
    "Lexicon_Score_test = test_df[['ID']].copy()\n",
    "Lexicon_Score_test['lexicon_score'] = test_df['unique_roots'].apply(get_lexicon_score)\n",
    "Lexicon_Score_test['lexicon_score_log'] = test_df['unique_roots'].apply(get_max_log_odds)\n",
    "#save to csv\n",
    "Lexicon_Score_train.to_csv('train_with_lexicon_verb.csv', index=False)\n",
    "Lexicon_Score_test.to_csv('test_with_lexicon_verb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd4b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_meltingpoint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
