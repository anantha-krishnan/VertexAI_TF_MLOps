{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"},{"sourceId":13815217,"sourceType":"datasetVersion","datasetId":8797290}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T11:58:51.469494Z","iopub.execute_input":"2025-11-23T11:58:51.469727Z","iopub.status.idle":"2025-11-23T11:58:53.228181Z","shell.execute_reply.started":"2025-11-23T11:58:51.469704Z","shell.execute_reply":"2025-11-23T11:58:53.227301Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip\n/kaggle/input/classification-dataset-unzipped/test_labels.csv\n/kaggle/input/classification-dataset-unzipped/train.csv\n/kaggle/input/classification-dataset-unzipped/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile cfg.py\nimport sys\nimport os\nfrom pathlib import Path\n# 1. Force Legacy Mode\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n\n# 2. Import tf_keras first\nimport tf_keras\n\n# 3. THE TRICK: Tell Python that 'keras' is actually 'tf_keras'\n# This forces keras_tuner to use tf_keras internally.\nsys.modules[\"keras\"] = tf_keras\nimport tf_keras as keras \nimport tensorflow as tf\n\nclass CFG:\n    #current_dir = Path(__file__).resolve().parent\n    BASE_PATH = '/kaggle/input/classification-dataset-unzipped'\n    seed = 42  # Random seed\n    preset = \"distilbert-base-uncased\"#\"roberta-base\"# \"deberta_v3_extra_small_en\" # Name of pretrained models\n    sequence_length = 256  # Input sequence length\n    epochs = 5 # Training epochs\n    batch_size = 256  # Batch size\n    scheduler = 'cosine'  # Learning rate scheduler\n    label_cols = [\n        'toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']  # Target labels\n    text_col = 'comment_text'  # Input text column\n    metrics = [\n        keras.metrics.BinaryAccuracy(name='accuracy'),\n        keras.metrics.AUC(name='auc', multi_label=True)\n    ]  # Evaluation metrics\n    shuffle = True  # Shuffle dataset\n    alpha = 0.25  # Focal loss alpha parameter\n    gamma = 2.0  # Focal loss gamma parameter\n    n_splits = 3  # Number of folds for cross-validation\n    learning_rate = 3e-5  # Learning rate\n    weight_decay = 1e-6  # Weight decay\n    warmup_ratio = 0.1  # Warmup ratio for learning rate scheduler\n    max_grad_norm = 1.0  # Maximum gradient norm for clipping\n    dropout_rate = 0.3  # Dropout rate for regularization\n    hidden_size = 256  # Hidden layer size\n    dense_size = 128  # Dense layer size\n    tuner_epochs = 2  # Number of epochs for hyperparameter tuning\n    tuner_batch_size = 8  # Batch size for hyperparameter tuning\n    tuner_trials = 5  # Number of trials for hyperparameter tuning\n    tuner_executions_per_trial = 1  # Executions per trial for hyperparameter tuning\n    model_dir = './model_checkpoints'  # Directory to save model checkpoints\n    submission_file = './submission.csv'  # Path to save submission file\n    pretrained_dir = './pretrained_models'  # Directory to save pretrained models\n    log_dir = './logs'  # Directory for TensorBoard logs\n    use_amp = True  # Use Automatic Mixed Precision\n    device = 'cuda' if tf.config.list_physical_devices('GPU') else 'cpu'  # Device configuration\n    num_workers = 4  # Number of workers for data loading\n    pin_memory = True  # Pin memory for data loading\n    early_stopping_patience = 3  # Early stopping patience\n    early_stopping_monitor = 'val_loss'  #'val_auc'# Metric to monitor for early stopping\n    early_stopping_mode = 'min'  #'max'# Mode for early stopping ('min' or 'max')\n    early_stopping_restore_best_weights = True  # Restore best weights on early stopping\n    random_state = 42  # Random state for reproducibility\n    verbose = 1  # Verbosity level\n    save_best_only = True  # Save only the best model\n    save_weights_only = False  # Save the entire model, not just weights\n    save_freq = 'epoch'  # Frequency to save the model\n    monitor_metric = 'val_loss'  # Metric to monitor for saving the model\n    n_unfreeze=3\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T11:58:53.229546Z","iopub.execute_input":"2025-11-23T11:58:53.229908Z","iopub.status.idle":"2025-11-23T11:58:53.237206Z","shell.execute_reply.started":"2025-11-23T11:58:53.229886Z","shell.execute_reply":"2025-11-23T11:58:53.236159Z"}},"outputs":[{"name":"stdout","text":"Writing cfg.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile jigsaw_classifier_updates.py\nimport sys\nimport os\n\n# 1. Force Legacy Mode\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n\n# 2. Import tf_keras first\nimport tf_keras\n\n# 3. THE TRICK: Tell Python that 'keras' is actually 'tf_keras'\n# This forces keras_tuner to use tf_keras internally.\nsys.modules[\"keras\"] = tf_keras\n\n# 4. NOW import the rest\nimport tensorflow as tf\nimport numpy as np\nimport keras_tuner as kt\nfrom transformers import AutoTokenizer, TFAutoModel\n# You can now refer to tf_keras simply as keras, or keep your alias\nimport tf_keras as keras \nfrom tf_keras.callbacks import EarlyStopping\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport seaborn as sns # Optional, just makes the colors prettier\nfrom sklearn.model_selection import KFold\nimport tf_keras.backend as K # To clear memory\nimport gc # Garbage collector\n# \n#from peft import LoraConfig, get_peft_model\nfrom tensorflow.keras import mixed_precision\nfrom tf_keras.callbacks import ReduceLROnPlateau\n# local imports\nfrom cfg import CFG\n\nclass JigsawClassifier:\n    def __init__(self, CFG=CFG):\n        self.CFG = CFG\n        self.train_df, self.test_combined_df, test_combined_cleaned_df = self.preprocess_input_csv()\n        test_combined_cleaned_02 = test_combined_cleaned_df.sample(frac=0.2, random_state=42).reset_index(drop=True)\n        self.test_df = self.test_combined_df\n        \n        if CFG.device == 'cpu':\n            #self.test_combined_cleaned_df = test_combined_cleaned_02\n            self.steps_per_epoch = 100\n            self.validation_steps = 20\n        else:\n            self.steps_per_epoch = len(self.train_df) // self.CFG.batch_size\n            self.validation_steps = len(self.test_df) // self.CFG.batch_size\n            #self.steps_per_epoch = 200\n            #self.validation_steps = 40\n            policy = mixed_precision.Policy('mixed_float16')\n            mixed_precision.set_global_policy(policy)\n            print(\"Mixed Precision (float16) enabled.\")\n        self.model = self.build_model_tuner()\n        self.compile_model()\n        self.tokenizer = AutoTokenizer.from_pretrained(self.CFG.preset)\n        self.train_ds, self.val_ds, self.test_ds = self.create_data_loader()\n\n        # outputs\n        self.y_pred = None\n\n    def get_lr_callback(self):\n        return ReduceLROnPlateau(\n            monitor=self.CFG.early_stopping_monitor,\n            patience=self.CFG.early_stopping_patience,\n            mode=self.CFG.early_stopping_mode,            \n            factor=0.5,            # Cut LR by half (multiply by 0.5)            \n            min_lr=self.CFG.learning_rate,           # Don't go below this\n            verbose=1              # Print message when LR changes\n        )\n    def create_dataset(self, df, shuffle=None):\n        if shuffle is None:\n            shuffle = self.CFG.shuffle\n        texts = df[self.CFG.text_col]\n        labels = df[self.CFG.label_cols] if self.CFG.label_cols is not None else None\n        encodings = self.tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding='max_length',\n            max_length=self.CFG.sequence_length,\n            return_tensors='tf'\n        )\n        if labels is not None:\n            dataset = tf.data.Dataset.from_tensor_slices((\n                {\n                    'input_ids': encodings['input_ids'],\n                    'attention_mask': encodings['attention_mask']\n                },\n                labels.values\n            ))\n        else:\n            dataset = tf.data.Dataset.from_tensor_slices({\n                'input_ids': encodings['input_ids'],\n                'attention_mask': encodings['attention_mask']\n            })\n        dataset = dataset.cache() \n        if shuffle:\n            dataset = dataset.shuffle(buffer_size=len(texts), seed=self.CFG.seed)\n        dataset = dataset.batch(self.CFG.batch_size)\n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        return dataset\n    \n    def preprocess_input_csv(self):\n        if self.CFG.BASE_PATH is None:\n            raise ValueError(\"BASE_PATH is not set in CFG.\")\n        train_df = pd.read_csv(f'{self.CFG.BASE_PATH}/train.csv')\n        test_df = pd.read_csv(f'{self.CFG.BASE_PATH}/test.csv')\n        test_labels_df = pd.read_csv(f'{self.CFG.BASE_PATH}/test_labels.csv')\n        test_combined_df = pd.merge(test_df, test_labels_df, on='id')\n        test_combined_cleaned_df = test_combined_df[~test_combined_df[self.CFG.label_cols].isin([-1]).any(axis=1)]\n        return train_df, test_combined_df, test_combined_cleaned_df\n    \n    def set_trainable_params(self, base_model):\n        \"\"\"\n        Calculates and prints the number of trainable parameters.\n        \"\"\"\n        # ============================================================\n        # CRITICAL FIX: \n        # The Top-Level Model MUST be trainable. \n        # We will manually freeze the sub-components we don't want.\n        # ============================================================\n        base_model.trainable = True \n        \n        # 1. LOCATE SUB-COMPONENTS\n        # We need to find two things: \n        #   A. The Embeddings (Always Freeze)\n        #   B. The Encoder Layers (Selectively Freeze)\n        \n        if hasattr(base_model, 'distilbert'):\n            embeddings = base_model.distilbert.embeddings\n            transformer_layers = base_model.distilbert.transformer.layer\n            model_type = \"DistilBERT\"\n        elif hasattr(base_model, 'roberta'):\n            embeddings = base_model.roberta.embeddings\n            transformer_layers = base_model.roberta.encoder.layer\n            model_type = \"RoBERTa\"\n        elif hasattr(base_model, 'bert'):\n            embeddings = base_model.bert.embeddings\n            transformer_layers = base_model.bert.encoder.layer\n            model_type = \"BERT\"\n        else:\n            # Generic Fallback (Might miss embeddings, but safer than crashing)\n            # Usually layer[0] is the main trunk\n            embeddings = None \n            transformer_layers = base_model.layers[0].encoder.layer\n            model_type = \"Unknown\"\n\n        # 2. ALWAYS FREEZE EMBEDDINGS\n        # (The dictionary of words should not change)\n        if embeddings is not None:\n            embeddings.trainable = False\n\n        # 3. SELECTIVELY FREEZE LAYERS\n        n_to_unfreeze = self.CFG.n_unfreeze\n        total_layers = len(transformer_layers)\n        cutoff = total_layers - n_to_unfreeze\n\n        print(f\"--- Optimizing {model_type} ---\")\n        print(f\"Freezing Embeddings: Yes\")\n        print(f\"Freezing Bottom {cutoff} Layers\")\n        print(f\"Unfreezing Top {n_to_unfreeze} Layers\")\n\n        for i, layer in enumerate(transformer_layers):\n            if i < cutoff:\n                layer.trainable = False # Freeze Bottom\n            else:\n                layer.trainable = True  # Train Top\n\n        # 4. CALCULATE STATS\n        # We force Keras to re-evaluate the weights by accessing .trainable_weights\n        trainable_count = np.sum([K.count_params(w) for w in base_model.trainable_weights])\n        non_trainable_count = np.sum([K.count_params(w) for w in base_model.non_trainable_weights])\n        total_count = trainable_count + non_trainable_count\n        \n        if total_count == 0:\n            percentage = 0\n        else:\n            percentage = (trainable_count / total_count) * 100\n        \n        print(\"\\n\" + \"=\"*40)\n        print(f\"base_model PARAMETER STATS\")\n        print(\"=\"*40)\n        print(f\"Total Params:        {total_count:,.0f}\")\n        print(f\"Trainable Params:    {trainable_count:,.0f}\")\n        print(f\"Non-Trainable Params:{non_trainable_count:,.0f}\")\n        print(f\"Trainable Percentage: {percentage:.2f}%\")\n        print(\"=\"*40 + \"\\n\")\n        \n        return base_model\n        \n    def build_model_tuner(self,hp=None):\n        if hp is None:\n            dropout_rate = self.CFG.dropout_rate\n            learning_rate = self.CFG.learning_rate\n        else:\n            dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n            learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n        \n        # Using 'keras' here (which is actually tf_keras now)\n        input_ids = keras.layers.Input(shape=(self.CFG.sequence_length,), dtype=tf.int32, name='input_ids')\n        attention_mask = keras.layers.Input(shape=(self.CFG.sequence_length,), dtype=tf.int32, name='attention_mask')\n        \n        base_model = TFAutoModel.from_pretrained(self.CFG.preset)\n        #lora_config = LoraConfig(\n        #    r=8,                    # Rank 8 is sufficient for Classification\n        #    lora_alpha=16,          # Alpha should be 2x Rank\n        #    target_modules=[\"query\", \"value\"], # These are the attention matrices in BERT/RoBERTa\n        #    lora_dropout=0.1,\n        #    bias=\"none\",\n        #)\n        #base_model = get_peft_model(base_model, lora_config)\n        #print(\"\\n--- LoRA Activated ---\")\n        \n        base_model = self.set_trainable_params(base_model)\n        \n        output = base_model(input_ids, attention_mask=attention_mask)\n        pooled_output = output.last_hidden_state[:, 0, :]\n        \n        dropout = keras.layers.Dropout(dropout_rate, name='dropout')(pooled_output)\n        output = keras.layers.Dense(len(self.CFG.label_cols), \n                                    activation='sigmoid',\n                                    name='sigmoid_output',\n                                    dtype='float32',)(dropout)\n        \n        model = keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n        return model\n    def compile_model(self, hp=None):\n        if hp is None:\n            learning_rate = self.CFG.learning_rate\n        else:\n            learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n        \n         # Using 'keras' here (which is actually tf_keras now)\n        self.model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n            #loss='binary_crossentropy',\n            loss=self.focull_loss,\n            metrics=self.CFG.metrics,\n            jit_compile=True\n        )\n        \n    def early_stopping_callback(self):\n        # Using 'keras' here (which is actually tf_keras now)\n        return EarlyStopping(\n            monitor=self.CFG.early_stopping_monitor,\n            patience=self.CFG.early_stopping_patience,\n            mode=self.CFG.early_stopping_mode,\n            restore_best_weights=self.CFG.early_stopping_restore_best_weights\n        )\n    def get_tuner(self):\n        tuner = kt.RandomSearch(\n            hypermodel=self.build_model_tuner,\n            objective=kt.Objective(\"val_loss\", direction=\"min\"),\n            max_trials=2,\n            executions_per_trial=1,\n            overwrite=True,\n            directory='kt_tuner_dir',\n            project_name='jigsaw_classifier_tuning'\n        )\n\n        print(\"Search space summary:\")\n        tuner.search_space_summary()\n        return tuner\n    \n    def create_data_loader(self):\n        train_split_df, val_split_df = train_test_split(\n            self.train_df,\n            test_size=0.2,\n            random_state=self.CFG.seed,\n        )\n        val_split_ds = self.create_dataset(\n            val_split_df, shuffle=self.CFG.shuffle\n        )\n        train_split_ds = self.create_dataset(\n            train_split_df, shuffle=self.CFG.shuffle)\n        \n        test_ds = self.create_dataset(\n            self.test_df, shuffle=False    \n        )\n        return train_split_ds, val_split_ds, test_ds\n    \n    def train_model(self):\n        early_stopping = self.early_stopping_callback()\n\n        history = self.model.fit(\n            self.train_ds,\n            validation_data=self.val_ds,\n            epochs=self.CFG.epochs,\n            callbacks=[early_stopping],\n            steps_per_epoch=self.steps_per_epoch,   \n            validation_steps=self.validation_steps  \n        )\n        return history\n    def predict(self):\n        self.y_pred  = self.model.predict(self.test_ds)\n        return self.y_pred\n    def focull_loss(self, y_true, y_pred):\n        alpha = self.CFG.alpha \n        gamma = self.CFG.gamma\n        \n        # Cast to float32 to avoid type errors\n        y_true = tf.cast(y_true, tf.float32)\n        y_pred = tf.cast(y_pred, tf.float32)\n        \n        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1. - tf.keras.backend.epsilon())\n        \n        # Masking logic\n        # If y_true is 0, set p_t1 to 1 so log(1)=0 (cancels out)\n        p_t1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        \n        # If y_true is 1, set p_t0 to 0 so log(1-0)=0 (cancels out)\n        p_t0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        \n        # --- CHANGES START HERE ---\n        \n        # REMOVE tf.reduce_sum. Keep the shape as (batch_size, labels)\n        p = - (alpha * tf.pow(1. - p_t1, gamma) * tf.math.log(p_t1))\n        n = - ((1. - alpha) * tf.pow(p_t0, gamma) * tf.math.log(1. - p_t0))\n        \n        # NOW you can use mean. \n        # It adds p+n (element-wise) and then averages over the batch.\n        return tf.reduce_mean(p + n)\n    \n    def k_fold_model_training(self):\n        # Implement k-fold cross-validation training here\n        kf = KFold(n_splits=self.CFG.n_splits, shuffle=True, random_state=self.CFG.random_state)\n        for fold, (train_index, val_index) in enumerate(kf.split(self.train_df)):\n            K.clear_session()\n            gc.collect()\n            train_fold_df = self.train_df.iloc[train_index]\n            val_fold_df = self.train_df.iloc[val_index]\n            self.train_ds = self.create_dataset(train_fold_df, shuffle=self.CFG.shuffle)\n            self.val_ds = self.create_dataset(val_fold_df, shuffle=self.CFG.shuffle)\n            self.model = self.build_model_tuner()\n            self.compile_model()\n            early_stopping = self.early_stopping_callback()\n            self.steps_per_epoch = len(train_fold_df) // self.CFG.batch_size\n            self.validation_steps = len(val_fold_df) // self.CFG.batch_size\n            self.model.fit(\n                self.train_ds,\n                validation_data=self.val_ds,\n                epochs=self.CFG.epochs,\n                callbacks=[early_stopping],\n                #steps_per_epoch=self.steps_per_epoch,\n                #validation_steps=self.validation_steps\n            )\n            # After training on this fold, make predictions on the test set\n            fold_y_pred = self.model.predict(self.test_ds)\n            if self.y_pred is None:\n                self.y_pred = fold_y_pred / self.CFG.n_splits\n            else:\n                self.y_pred += fold_y_pred / self.CFG.n_splits\n            # --- Cleanup to save RAM ---\n            del self.model, self.train_ds, self.val_ds\n        y_pred_df = pd.DataFrame(self.y_pred, columns=self.CFG.label_cols)\n        # insert the 'id' column from test_combined_cleaned dataframe into y_pred_df at the beginning\n        y_pred_df.insert(0, 'id', self.test_combined_df['id'].values)\n        y_pred_df.to_csv('submission.csv', index=False)\n            \n        \n    def plot_metrics(self ):        \n        # Set up the plot\n        plt.figure(figsize=(10, 8))\n        colors = sns.color_palette(\"bright\", n_colors=len(CFG.label_cols))\n        lw = 2 # Line width\n        test_combined_cleaned = self.test_combined_df[~self.test_combined_df[CFG.label_cols].isin([-1]).any(axis=1)]\n        y_pred_cleaned = self.y_pred[~self.test_combined_df[CFG.label_cols].isin([-1]).any(axis=1)]\n\n        # Loop through each label (Toxic, Severe_Toxic, etc.)\n        for i, label in enumerate(CFG.label_cols):\n            # 1. Compute FPR and TPR for this specific label\n            \n    \n            fpr, tpr, thresholds  = roc_curve(test_combined_cleaned[label].values, y_pred_cleaned[:, i])\n            J = tpr - fpr\n            ix = np.argmax(J) # Index of the maximum J\n            best_thresh = thresholds[ix]\n            best_fpr = fpr[ix]\n            best_tpr = tpr[ix]\n            max_j = J[ix]\n            print(f\"{label:<15}: Best Thresh={best_thresh:.3f}, Max J={max_j:.3f}\")\n            # 2. Calculate the AUC score for this specific label\n            roc_auc = auc(fpr, tpr)\n            plt.scatter(best_fpr, best_tpr, color=colors[i], s=70, edgecolor='black', zorder=5)\n            offset_y = -20 - (i * 12)\n            # 3. Plot the curve\n            plt.plot(fpr, tpr, color=colors[i], lw=lw,\n                    label=f'{label} (area = {roc_auc:.2f})')\n            plt.annotate(f'Th={best_thresh:.2f}', \n                        xy=(best_fpr, best_tpr), \n                        xytext=(20, offset_y), # Offset text to the right and down\n                        textcoords='offset points',\n                        fontsize=9, \n                        arrowprops=dict(arrowstyle=\"->\", color='gray', alpha=0.5),\n                        color=colors[i],\n                        fontweight='bold')\n\n        # Plot the \"Random Guess\" line (diagonal)\n        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n\n        # Formatting\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate (FPR)')\n        plt.ylabel('True Positive Rate (TPR)')\n        plt.title('ROC Curves by Toxicity Type')\n        plt.legend(loc=\"lower right\")\n        plt.grid(True, alpha=0.3)\n\n        plt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T11:58:53.238154Z","iopub.execute_input":"2025-11-23T11:58:53.239332Z","iopub.status.idle":"2025-11-23T11:58:53.269380Z","shell.execute_reply.started":"2025-11-23T11:58:53.239308Z","shell.execute_reply":"2025-11-23T11:58:53.268596Z"}},"outputs":[{"name":"stdout","text":"Writing jigsaw_classifier_updates.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from jigsaw_classifier_updates import JigsawClassifier\nfrom cfg import CFG\n\n#if __name__ == \"__main__\":\nclassifier = JigsawClassifier(CFG)\nclassifier.k_fold_model_training()\n    #y_pred = classifier.predict()\nclassifier.plot_metrics()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T11:58:53.270875Z","iopub.execute_input":"2025-11-23T11:58:53.271145Z"}},"outputs":[{"name":"stderr","text":"2025-11-23 11:58:54.903836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763899135.092745      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763899135.149310      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"I0000 00:00:1763899160.642176      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\nYour GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n  Tesla P100-PCIE-16GB, compute capability 6.0\nSee https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\nIf you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\nMixed Precision (float16) enabled.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d31a0ab3c142d89de11c4a4de3b49c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e398dea94f894639a28bd330626d0146"}},"metadata":{}},{"name":"stderr","text":"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFDistilBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"--- Optimizing DistilBERT ---\nFreezing Embeddings: Yes\nFreezing Bottom 3 Layers\nUnfreezing Top 3 Layers\n\n========================================\nbase_model PARAMETER STATS\n========================================\nTotal Params:        66,362,880\nTrainable Params:    21,263,616\nNon-Trainable Params:45,099,264\nTrainable Percentage: 32.04%\n========================================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb3fe87ab8b4846a38929a822217edb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a2c3863a6de406596d66591fb905917"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6de8657c0f2843be832d6b2abc723612"}},"metadata":{}},{"name":"stderr","text":"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFDistilBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"--- Optimizing DistilBERT ---\nFreezing Embeddings: Yes\nFreezing Bottom 3 Layers\nUnfreezing Top 3 Layers\n\n========================================\nbase_model PARAMETER STATS\n========================================\nTotal Params:        66,362,880\nTrainable Params:    21,263,616\nNon-Trainable Params:45,099,264\nTrainable Percentage: 32.04%\n========================================\n\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1763899264.232334     105 service.cc:148] XLA service 0x7a60bc0019d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1763899264.233365     105 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nW0000 00:00:1763899264.655965     105 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\nI0000 00:00:1763899266.671875     105 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1763899285.082477     105 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"415/416 [============================>.] - ETA: 1s - loss: 0.0079 - accuracy: 0.9739 - auc: 0.9133","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763899944.203587     103 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"416/416 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9739 - auc: 0.9134","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763899963.418970     104 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\nW0000 00:00:1763900123.138103     105 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"416/416 [==============================] - 876s 2s/step - loss: 0.0079 - accuracy: 0.9739 - auc: 0.9134 - val_loss: 0.0048 - val_accuracy: 0.9808 - val_auc: 0.9835\nEpoch 2/5\n416/416 [==============================] - 817s 2s/step - loss: 0.0058 - accuracy: 0.9788 - auc: 0.9574 - val_loss: 0.0048 - val_accuracy: 0.9798 - val_auc: 0.9854\nEpoch 3/5\n416/416 [==============================] - 818s 2s/step - loss: 0.0056 - accuracy: 0.9792 - auc: 0.9661 - val_loss: 0.0048 - val_accuracy: 0.9770 - val_auc: 0.9874\nEpoch 4/5\n416/416 [==============================] - 818s 2s/step - loss: 0.0055 - accuracy: 0.9794 - auc: 0.9672 - val_loss: 0.0047 - val_accuracy: 0.9779 - val_auc: 0.9871\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763903400.809812     104 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"598/599 [============================>.] - ETA: 0s","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763903858.721528     105 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"599/599 [==============================] - 462s 767ms/step\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFDistilBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"--- Optimizing DistilBERT ---\nFreezing Embeddings: Yes\nFreezing Bottom 3 Layers\nUnfreezing Top 3 Layers\n\n========================================\nbase_model PARAMETER STATS\n========================================\nTotal Params:        66,362,880\nTrainable Params:    21,263,616\nNon-Trainable Params:45,099,264\nTrainable Percentage: 32.04%\n========================================\n\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763903902.579210     103 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"415/416 [============================>.] - ETA: 1s - loss: 0.0102 - accuracy: 0.9718 - auc: 0.9262","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763904569.907607     104 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"416/416 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9718 - auc: 0.9262","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763904588.506658     104 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\nW0000 00:00:1763904748.101808     105 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"416/416 [==============================] - 863s 2s/step - loss: 0.0102 - accuracy: 0.9718 - auc: 0.9262 - val_loss: 0.0051 - val_accuracy: 0.9786 - val_auc: 0.9822\nEpoch 2/5\n416/416 [==============================] - 817s 2s/step - loss: 0.0060 - accuracy: 0.9785 - auc: 0.9544 - val_loss: 0.0048 - val_accuracy: 0.9816 - val_auc: 0.9838\nEpoch 3/5\n416/416 [==============================] - 817s 2s/step - loss: 0.0058 - accuracy: 0.9793 - auc: 0.9639 - val_loss: 0.0045 - val_accuracy: 0.9821 - val_auc: 0.9864\nEpoch 4/5\n416/416 [==============================] - 817s 2s/step - loss: 0.0058 - accuracy: 0.9791 - auc: 0.9659 - val_loss: 0.0046 - val_accuracy: 0.9798 - val_auc: 0.9864\nEpoch 5/5\n416/416 [==============================] - 817s 2s/step - loss: 0.0056 - accuracy: 0.9793 - auc: 0.9730 - val_loss: 0.0044 - val_accuracy: 0.9827 - val_auc: 0.9872\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763908022.212594     102 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"598/599 [============================>.] - ETA: 0s","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763908479.799685     105 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"599/599 [==============================] - 460s 765ms/step\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFDistilBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"--- Optimizing DistilBERT ---\nFreezing Embeddings: Yes\nFreezing Bottom 3 Layers\nUnfreezing Top 3 Layers\n\n========================================\nbase_model PARAMETER STATS\n========================================\nTotal Params:        66,362,880\nTrainable Params:    21,263,616\nNon-Trainable Params:45,099,264\nTrainable Percentage: 32.04%\n========================================\n\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763908520.154585     104 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"415/416 [============================>.] - ETA: 1s - loss: 0.0087 - accuracy: 0.9755 - auc: 0.9433","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763909186.793541     103 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"416/416 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9755 - auc: 0.9433","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1763909199.109244     104 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\nW0000 00:00:1763909358.463862     102 assert_op.cc:38] Ignoring Assert operator model/tf_distil_bert_model/distilbert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"416/416 [==============================] - 851s 2s/step - loss: 0.0086 - accuracy: 0.9755 - auc: 0.9433 - val_loss: 0.0054 - val_accuracy: 0.9764 - val_auc: 0.9829\nEpoch 2/5\n257/416 [=================>............] - ETA: 4:11 - loss: 0.0058 - accuracy: 0.9788 - auc: 0.9658","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}