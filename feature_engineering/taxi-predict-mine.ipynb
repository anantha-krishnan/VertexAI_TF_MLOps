{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column as fc\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "train_ds = 'D:/PaperWork/personal/AI/VertexAI/training-data-analyst/courses/machine_learning/deepdive2/feature_engineering/labs/../../../data/traffic-taxi-train.csv'\n",
    "test_ds = 'D:/PaperWork/personal/AI/VertexAI/training-data-analyst/courses/machine_learning/deepdive2/feature_engineering/labs/../../../data/traffic-taxi-test.csv'\n",
    "validation_ds = 'D:/PaperWork/personal/AI/VertexAI/training-data-analyst/courses/machine_learning/deepdive2/feature_engineering/labs/../../../data/traffic-taxi-valid.csv'\n",
    "\n",
    "csv_cols = ['fare_amount',\n",
    "                'pickup_datetime',\n",
    "                'pickup_longitude',\n",
    "                'pickup_latitude',\n",
    "                'dropoff_longitude',\n",
    "                'dropoff_latitude',\n",
    "                'passenger_count',\n",
    "                'trips_last_5min',\n",
    "                'key']\n",
    "\n",
    "num_cols = ['pickup_longitude',\n",
    "            'pickup_latitude',\n",
    "            'dropoff_longitude',\n",
    "            'dropoff_latitude',\n",
    "            'passenger_count']\n",
    "string_cols = ['pickup_datetime']\n",
    "label_col = 'fare_amount'\n",
    "unwanted_cols = ['key','trips_last_5min']\n",
    "defaults = [[0.0], [''], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "DAYS = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat','Sun']\n",
    "DAYS_DICT = {idx: day for idx, day in enumerate(DAYS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print the parent directory of the current file\n",
    "def read_data(file_path):    \n",
    "    df = pd.read_csv(file_path, names=csv_cols, header=0, na_values='', dtype={'fare_amount': 'float32',\n",
    "                                                                           'pickup_datetime': 'string',\n",
    "                                                                           'pickup_longitude': 'float32',\n",
    "                                                                           'pickup_latitude': 'float32',\n",
    "                                                                           'dropoff_longitude': 'float32',\n",
    "                                                                           'dropoff_latitude': 'float32',\n",
    "                                                                           'passenger_count': 'float32',\n",
    "                                                                           'trips_last_5min': 'float32',\n",
    "                                                                           'key': 'float32'})\n",
    "    for col in unwanted_cols:\n",
    "        df.pop(col)\n",
    "    return df\n",
    "def convert_pickup_datetime(dt):    \n",
    "    ts = parser.parse(dt)\n",
    "    return DAYS_DICT.get(ts.weekday(),'Unknown'), ts.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins=5\n",
    "df= read_data(train_ds)\n",
    "longs = np.concatenate([df['pickup_longitude'], df['dropoff_longitude']], axis=0).flatten()\n",
    "lats = np.concatenate([df['pickup_latitude'], df['dropoff_latitude']], axis=0).flatten()\n",
    "discretizer_long = layers.Discretization(name='discretize_longitude', num_bins=num_bins)\n",
    "discretizer_lat = layers.Discretization(name='discretize_latitude', num_bins=num_bins)\n",
    "discretizer_long.adapt(longs)\n",
    "discretizer_lat.adapt(lats)\n",
    "\n",
    "# print(df.head())\n",
    "# print(df.describe())\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "def haversine_distance(coords):\n",
    "    lat1, lon1, lat2, lon2 = coords\n",
    "    PI = tf.constant(np.pi, dtype=tf.float32)\n",
    "    R = tf.constant(6371.0, dtype=tf.float32)  # Earth radius in kilometers\n",
    "    lat1_rad = lat1 * (PI / 180.0)\n",
    "    lon1_rad = lon1 * (PI / 180.0)\n",
    "    lat2_rad = lat2 * (PI / 180.0)\n",
    "    lon2_rad = lon2 * (PI / 180.0)\n",
    "\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "\n",
    "    a = tf.math.sin(dlat / 2) ** 2 + tf.math.cos(lat1_rad) * tf.math.cos(lat2_rad) * tf.math.sin(dlon / 2) ** 2\n",
    "    c = 2 * tf.asin(tf.sqrt(a))\n",
    "\n",
    "    distance = R * c\n",
    "    distance.set_shape([None])\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "def build_model(h1_units=128, h2_units=64, dropout_rate=0.3, learning_rate=0.0001):\n",
    "    \n",
    "    inputs = {col: layers.Input(name=col, shape=(), dtype='float32') for col in num_cols}\n",
    "    inputs.update({col: layers.Input(name=col, shape=(), dtype='string') for col in string_cols})\n",
    "    dt_inp = inputs['pickup_datetime']\n",
    "    def debug_print(tensor):\n",
    "        tf.print(\"\\n[DEBUG] Pickup datetime tensor seen by model: \", tensor, summarize=-1)\n",
    "        return tensor\n",
    "\n",
    "    #dt_inp = layers.Lambda(debug_print, output_shape=(), name=\"debug_print\")(dt_inp_original)\n",
    "    # 1. PROCESS NUMERICAL FEATURES\n",
    "    reshaped_inputs = {col: layers.Reshape(name='reshape_' + col, target_shape=(1,))(inputs[col]) for col in num_cols}\n",
    "    feature_layer = layers.concatenate(list(reshaped_inputs.values()), name='concatenate_numerical_features')\n",
    "\n",
    "    # normalize numerical features\n",
    "    normalizer = layers.Normalization(name='normalizer')\n",
    "    normalizer.adapt(df[num_cols].to_numpy())\n",
    "    normalized_feature_layer = normalizer(feature_layer)\n",
    "    \n",
    "    # 2. FEATURE ENGINEERING FROM DATETIME STRING\n",
    "    # prepare a python function to parse a batch of datetime strings into day of week and hour of day\n",
    "    # to be wrapped in a tf.py_function\n",
    "    def parse_datetime_batch(dt_tensor):\n",
    "        dts = [t.decode('utf-8') for t in dt_tensor.numpy()]\n",
    "        days = []\n",
    "        hours = []\n",
    "        for dt in dts:\n",
    "            day, hour = convert_pickup_datetime(dt)\n",
    "            days.append(day)\n",
    "            hours.append(hour)          \n",
    "        return np.array(days, dtype=np.str_), np.array(hours, dtype=np.float32)\n",
    "    \n",
    "    # wrap the python function in a tf.py_function\n",
    "    def parse_datetime_batch_wrapper(dt_tensor):\n",
    "        day_of_week_str, hour_of_day_num = tf.py_function(\n",
    "            func=parse_datetime_batch,\n",
    "            inp=[dt_tensor],\n",
    "            Tout=[tf.string, tf.float32]\n",
    "            )\n",
    "        day_of_week_str.set_shape([None])\n",
    "        hour_of_day_num.set_shape([None])\n",
    "        return day_of_week_str, hour_of_day_num\n",
    "    day_of_week_str, hour_of_day_num = layers.Lambda(parse_datetime_batch_wrapper, name='parse_datetime')(dt_inp)\n",
    "    haveersine_dis = layers.Lambda(haversine_distance, output_shape=(), name='haversine_distance')(\n",
    "        [inputs['pickup_latitude'],\n",
    "        inputs['pickup_longitude'],\n",
    "        inputs['dropoff_latitude'],\n",
    "        inputs['dropoff_longitude']]\n",
    "    )\n",
    "    reshape_haveersine_dis = layers.Reshape(name='reshape_haversine_distance', target_shape=(1,))(haveersine_dis)\n",
    "    # create a lookup layer for day of week\n",
    "    day_lookup_func = layers.StringLookup(name='day_lookup', vocabulary=DAYS)\n",
    "    day_lookup = day_lookup_func(day_of_week_str)\n",
    "    # make it one hot encoded\n",
    "    day_one_hot_func = layers.CategoryEncoding(name='day_one_hot', num_tokens=len(DAYS)+1, output_mode = 'one_hot')\n",
    "    day_one_hot = day_one_hot_func(day_lookup)\n",
    "    \n",
    "    # reshape the hour_of_day_num to be compatible for concatenation\n",
    "    reshape_hour_of_day = layers.Reshape(name='reshape_hour_of_day', target_shape=(1,))(hour_of_day_num)\n",
    "    # normalize hour of day\n",
    "    hour_normalizer = layers.Normalization(name='hour_normalizer')\n",
    "    hour_normalizer.adapt(np.arange(0,24).reshape(-1,1))\n",
    "    normalized_hour_of_day = hour_normalizer(reshape_hour_of_day)\n",
    "\n",
    "    pickup_latitude_discretized = discretizer_lat(inputs['pickup_latitude'])\n",
    "    pickup_longitude_discretized = discretizer_long(inputs['pickup_longitude'])\n",
    "    dropoff_latitude_discretized = discretizer_lat(inputs['dropoff_latitude'])\n",
    "    dropoff_longitude_discretized = discretizer_long(inputs['dropoff_longitude'])\n",
    "    # 3. BUILD INTERACTIONS BETWEEN FEATURES\n",
    "    pickup_loc = layers.HashedCrossing(name='pickup_latitude_x_longitude', num_bins=num_bins*num_bins)([pickup_latitude_discretized, pickup_longitude_discretized])\n",
    "    pickup_loc_embedding = layers.Embedding(name='pickup_loc_embedding', input_dim=num_bins*num_bins, output_dim=8)(pickup_loc)\n",
    "    \n",
    "    dropoff_loc = layers.HashedCrossing(name='dropoff_latitude_x_longitude', num_bins=num_bins*num_bins)([dropoff_latitude_discretized, dropoff_longitude_discretized])\n",
    "    dropoff_loc_embedding = layers.Embedding(name='dropoff_loc_embedding', input_dim=num_bins*num_bins, output_dim=8)(dropoff_loc)\n",
    "\n",
    "    pickup_dropoff_loc = layers.HashedCrossing(name='pickup_x_dropoff', num_bins=(num_bins*num_bins)**2)([pickup_loc, dropoff_loc])\n",
    "    pickup_dropoff_embedding = layers.Embedding(name='pickup_x_dropoff_embedding', input_dim=(num_bins*num_bins)**2, output_dim=16)(pickup_dropoff_loc)\n",
    "    # 4. ASSEMBLE FINAL FEATURE VECTOR\n",
    "    # Concatenate the processed numerical and engineered features\n",
    "    normalized_feature_layer = layers.concatenate([normalized_feature_layer, day_one_hot, normalized_hour_of_day, reshape_haveersine_dis,pickup_loc_embedding, dropoff_loc_embedding, pickup_dropoff_embedding], name='final_concatenate')\n",
    "    # 5. BUILD THE DNN MODEL\n",
    "    input_neurons = normalized_feature_layer.shape[-1]*2\n",
    "\n",
    "    # get the nearest power of 2 lesser than or equal to input_neurons\n",
    "    def nearest_power_of_2_lesser(n):\n",
    "        power = 1\n",
    "        while power <= n:\n",
    "            power *= 2\n",
    "        return power // 2\n",
    "\n",
    "    # get the nearest power of 2 greater than or equal to input_neurons\n",
    "    def nearest_power_of_2_greater(n):\n",
    "        power = 1\n",
    "        while power < n:\n",
    "            power *= 2\n",
    "        return power\n",
    "    def nearest_power_of_2(n):\n",
    "        lesser = nearest_power_of_2_lesser(n)\n",
    "        greater = nearest_power_of_2_greater(n)\n",
    "        if (n - lesser) < (greater - n):\n",
    "            return lesser\n",
    "        else:\n",
    "            return greater\n",
    "    h1 = layers.Dense(name='hidden_layer_1', units=h1_units, activation='relu')(normalized_feature_layer)\n",
    "    d1 = layers.Dropout(dropout_rate, name='dropout_1')(h1)\n",
    "    h2 = layers.Dense(name='hidden_layer_2', units=h2_units, activation='relu')(d1)\n",
    "    d2 = layers.Dropout(dropout_rate, name='dropout_2')(h2)\n",
    "    output = layers.Dense(name='output_layer', units=1, activation='linear')(d2)\n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['mse', rmse])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69899aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_label(row_data):\n",
    "    for col in unwanted_cols:\n",
    "        row_data.pop(col)\n",
    "    label = row_data.pop('fare_amount')\n",
    "    return row_data, label\n",
    "# This function will be applied to each batch as it's generated.\n",
    "def debug_print(row_data):\n",
    "    tf.print(\"\\\\n--- Data batch seen by model ---\")\n",
    "    for key, value in row_data.items():\n",
    "        # You can print the key as a string and the tensor value\n",
    "        tf.print(\"[DEBUG]\", key, \": \", value)\n",
    "    #print(row_data)\n",
    "    #features,label = row_data\n",
    "    # Use tf.print, which works inside the TensorFlow graph.\n",
    "    #tf.print(\"pickup_datetime dtype:\", tf.strings.as_string(features['pickup_datetime'].dtype))\n",
    "    # Print the first 3 values of the tensor to see what they look like\n",
    "    #tf.print(\"pickup_datetime values:\", features['pickup_datetime'][:3])\n",
    "    return row_data\n",
    "def fix_datetime_dtype(row_data):\n",
    "    # This uses tf.strings.as_string, which is robust for converting any type.\n",
    "    row_data['pickup_datetime'] = tf.strings.as_string(row_data['pickup_datetime'])\n",
    "    return row_data\n",
    "def ds_to_tf(file, batch_size=32, mode='train'):\n",
    "    # 1. Load the entire CSV into a pandas DataFrame using your robust function\n",
    "    df = read_data(file)\n",
    "    \n",
    "    # 2. Separate the label from the features\n",
    "    label = df.pop('fare_amount')\n",
    "    \n",
    "    # 3. Create a tf.data.Dataset directly from the pandas DataFrame and labels\n",
    "    # from_tensor_slices handles the conversion perfectly, respecting the dtypes.\n",
    "    # We convert the DataFrame of features into a dictionary of tensors.\n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices((dict(df), label))\n",
    "    \n",
    "    # 4. Apply shuffling and batching based on the mode\n",
    "    if mode == 'train':\n",
    "        tf_ds = tf_ds.shuffle(buffer_size=batch_size * 10).repeat()\n",
    "    \n",
    "    tf_ds = tf_ds.batch(batch_size)\n",
    "    tf_ds = tf_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return tf_ds\n",
    "#def ds_to_tf(file, batch_size=32, mode='train'):\n",
    "    ds = tf.data.experimental.make_csv_dataset(file,\n",
    "                                               batch_size=batch_size,\n",
    "                                               column_names=csv_cols,\n",
    "                                               column_defaults=defaults,\n",
    "                                               label_name=None,\n",
    "                                               num_epochs=1,\n",
    "                                               shuffle=False)\n",
    "    ds = ds.map(fix_datetime_dtype)\n",
    "\n",
    "    ds = ds.map(features_and_label)\n",
    "\n",
    "    if mode=='train':\n",
    "        ds = ds.shuffle(buffer_size=batch_size*10).repeat()\n",
    "        ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "    label = df.pop('fare_amount')\n",
    "    tf_ds = tf.data.Dataset.from_tensor_slices((dict(df), label))\n",
    "    tf_ds = tf_ds.shuffle(buffer_size=batch_size*10)\n",
    "    tf_ds = tf_ds.batch(batch_size)\n",
    "    tf_ds = tf_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return tf_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df = read_data(train_ds)\n",
    "evaluation_data_df = read_data(validation_ds)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 32 \n",
    "VALIDATION_BATCH_SIZE = 1000\n",
    "NUM_TRAIN_EXAMPLES = len(train_data_df)\n",
    "NUM_EVALS = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE)\n",
    "NUM_EVAL_EXAMPLES = len(evaluation_data_df)\n",
    "NUM_VALIDATIONS = NUM_EVAL_EXAMPLES // (VALIDATION_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94211ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for training and evaluation\n",
    "trainds = ds_to_tf(train_ds, TRAIN_BATCH_SIZE, 'train')\n",
    "evalds = ds_to_tf(validation_ds, VALIDATION_BATCH_SIZE, 'eval')\n",
    "\n",
    "# Define early stopping to prevent long, fruitless training runs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_rmse', \n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# --- HYPERPARAMETER TUNING: HIDDEN LAYER 1 UNITS ---\n",
    "\n",
    "# 1. Define the values we want to try\n",
    "h1_units_options = [8, 16, 32, 64, 128, 256]\n",
    "histories_h1 = {}\n",
    "\n",
    "print(\"--- Starting Tuning for Hidden Layer 1 Units ---\")\n",
    "for units in h1_units_options:\n",
    "    print(f\"\\nTraining with h1_units = {units}...\")\n",
    "    \n",
    "    # 2. Build the model with the current value\n",
    "    # We use the new parameterized function\n",
    "    model_tuned = build_model(h1_units=units)\n",
    "    \n",
    "    # 3. Train the model\n",
    "    history = model_tuned.fit(\n",
    "        trainds,\n",
    "        epochs=15,  # Train for a few more epochs to see the trend\n",
    "        validation_data=evalds,\n",
    "        steps_per_epoch=NUM_EVALS,\n",
    "        validation_steps=NUM_VALIDATIONS,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0  # Set to 0 to keep the output clean\n",
    "    )\n",
    "    \n",
    "    # 4. Store the results\n",
    "    histories_h1[f'units_{units}'] = history\n",
    "    print(f\"Finished training for {units} units. Best val_rmse: {min(history.history['val_rmse']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf2d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VISUALIZATION: HIDDEN LAYER 1 UNITS ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training curves\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, history in histories_h1.items():\n",
    "    plt.plot(history.history['val_rmse'], label=name)\n",
    "plt.title('Validation RMSE vs. Epochs for h1_units')\n",
    "plt.ylabel('Validation RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot best performance\n",
    "plt.subplot(1, 2, 2)\n",
    "best_rmses = [min(h.history['val_rmse']) for h in histories_h1.values()]\n",
    "unit_labels = [str(u) for u in h1_units_options]\n",
    "plt.bar(unit_labels, best_rmses)\n",
    "plt.title('Best Validation RMSE per h1_units')\n",
    "plt.ylabel('Best Validation RMSE')\n",
    "plt.xlabel('Number of Units in Hidden Layer 1')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fbb824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HYPERPARAMETER TUNING: DROPOUT RATE ---\n",
    "\n",
    "# 1. Define the values we want to try\n",
    "dropout_options = [0.1, 0.2, 0.3, 0.4]\n",
    "histories_dropout = {}\n",
    "\n",
    "# Use the best h1_units from the previous step (e.g., 128)\n",
    "best_h1_units = 16  # Replace with the actual best value found \n",
    "\n",
    "print(\"\\n\\n--- Starting Tuning for Dropout Rate ---\")\n",
    "for rate in dropout_options:\n",
    "    print(f\"\\nTraining with dropout_rate = {rate}...\")\n",
    "    \n",
    "    # 2. Build the model with the current value\n",
    "    model_tuned = build_model(h1_units=best_h1_units, dropout_rate=rate)\n",
    "    \n",
    "    # 3. Train the model\n",
    "    history = model_tuned.fit(\n",
    "        trainds,\n",
    "        epochs=15,\n",
    "        validation_data=evalds,\n",
    "        steps_per_epoch=NUM_EVALS,\n",
    "        validation_steps=NUM_VALIDATIONS,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # 4. Store the results\n",
    "    histories_dropout[f'rate_{rate}'] = history\n",
    "    print(f\"Finished training for {rate} rate. Best val_rmse: {min(history.history['val_rmse']):.4f}\")\n",
    "\n",
    "\n",
    "# --- VISUALIZATION: DROPOUT RATE ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training curves\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, history in histories_dropout.items():\n",
    "    plt.plot(history.history['val_rmse'], label=name)\n",
    "plt.title('Validation RMSE vs. Epochs for Dropout Rate')\n",
    "plt.ylabel('Validation RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot best performance\n",
    "plt.subplot(1, 2, 2)\n",
    "best_rmses_dropout = [min(h.history['val_rmse']) for h in histories_dropout.values()]\n",
    "dropout_labels = [str(r) for r in dropout_options]\n",
    "plt.bar(dropout_labels, best_rmses_dropout)\n",
    "plt.title('Best Validation RMSE per Dropout Rate')\n",
    "plt.ylabel('Best Validation RMSE')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9530192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "# This function is the corrected version of build_keras_tuner_model\n",
    "def build_keras_tuner_model(hp):\n",
    "    # --- Define the Hyperparameter Search Space ---\n",
    "    h1_units = hp.Int('h1_units', min_value=8, max_value=40, step=8)\n",
    "    h2_units = hp.Int('h2_units', min_value=4, max_value=20, step=4)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "    \n",
    "    # --- The rest of your model architecture is the same ---\n",
    "    inputs = {col: layers.Input(name=col, shape=(), dtype='float32') for col in num_cols}\n",
    "    inputs.update({col: layers.Input(name=col, shape=(), dtype='string') for col in string_cols})\n",
    "    dt_inp = inputs['pickup_datetime']\n",
    "    \n",
    "    # 1. PROCESS NUMERICAL FEATURES\n",
    "    reshaped_inputs = {col: layers.Reshape(name='reshape_' + col, target_shape=(1,))(inputs[col]) for col in num_cols}\n",
    "    feature_layer = layers.concatenate(list(reshaped_inputs.values()), name='concatenate_numerical_features')\n",
    "\n",
    "    # CORRECTION 1: Use the two-step adapt method here\n",
    "    normalizer = layers.Normalization(name='normalizer')\n",
    "    normalizer.adapt(df[num_cols].to_numpy()) # df is the global DataFrame we loaded earlier\n",
    "    normalized_feature_layer = normalizer(feature_layer)\n",
    "    \n",
    "    # 2. FEATURE ENGINEERING\n",
    "    def parse_datetime_batch(dt_tensor):\n",
    "        dts = [t.decode('utf-8') for t in dt_tensor.numpy()]\n",
    "        days, hours = [], []\n",
    "        for dt in dts:\n",
    "            day, hour = convert_pickup_datetime(dt)\n",
    "            days.append(day); hours.append(hour)          \n",
    "        return np.array(days, dtype=np.str_), np.array(hours, dtype=np.float32)\n",
    "\n",
    "    def parse_datetime_batch_wrapper(dt_tensor):\n",
    "        day_of_week_str, hour_of_day_num = tf.py_function(func=parse_datetime_batch, inp=[dt_tensor], Tout=[tf.string, tf.float32])\n",
    "        day_of_week_str.set_shape([None]); hour_of_day_num.set_shape([None])\n",
    "        return day_of_week_str, hour_of_day_num\n",
    "        \n",
    "    day_of_week_str, hour_of_day_num = layers.Lambda(parse_datetime_batch_wrapper, name='parse_datetime')(dt_inp)\n",
    "    haveersine_dis = layers.Lambda(haversine_distance, name='haversine_distance')([inputs['pickup_latitude'], inputs['pickup_longitude'], inputs['dropoff_latitude'], inputs['dropoff_longitude']])\n",
    "    reshape_haveersine_dis = layers.Reshape(name='reshape_haversine_distance', target_shape=(1,))(haveersine_dis)\n",
    "    \n",
    "    day_lookup_func = layers.StringLookup(name='day_lookup', vocabulary=DAYS)\n",
    "    day_lookup = day_lookup_func(day_of_week_str)\n",
    "    day_one_hot_func = layers.CategoryEncoding(name='day_one_hot', num_tokens=len(DAYS)+1, output_mode = 'one_hot')\n",
    "    day_one_hot = day_one_hot_func(day_lookup)\n",
    "    \n",
    "    reshape_hour_of_day = layers.Reshape(name='reshape_hour_of_day', target_shape=(1,))(hour_of_day_num)\n",
    "\n",
    "    # CORRECTION 2: Use the two-step adapt method here as well\n",
    "    hour_normalizer = layers.Normalization(name='hour_normalizer')\n",
    "    hour_normalizer.adapt(np.arange(0,24).reshape(-1,1))\n",
    "    normalized_hour_of_day = hour_normalizer(reshape_hour_of_day)\n",
    "\n",
    "    pickup_latitude_discretized = discretizer_lat(inputs['pickup_latitude'])\n",
    "    pickup_longitude_discretized = discretizer_long(inputs['pickup_longitude'])\n",
    "    dropoff_latitude_discretized = discretizer_lat(inputs['dropoff_latitude'])\n",
    "    dropoff_longitude_discretized = discretizer_long(inputs['dropoff_longitude'])\n",
    "    \n",
    "    # 3. INTERACTIONS\n",
    "    pickup_loc = layers.HashedCrossing(name='pickup_latitude_x_longitude', num_bins=num_bins*num_bins)([pickup_latitude_discretized, pickup_longitude_discretized])\n",
    "    pickup_loc_embedding = layers.Embedding(name='pickup_loc_embedding', input_dim=num_bins*num_bins, output_dim=8)(pickup_loc)\n",
    "    dropoff_loc = layers.HashedCrossing(name='dropoff_latitude_x_longitude', num_bins=num_bins*num_bins)([dropoff_latitude_discretized, dropoff_longitude_discretized])\n",
    "    dropoff_loc_embedding = layers.Embedding(name='dropoff_loc_embedding', input_dim=num_bins*num_bins, output_dim=8)(dropoff_loc)\n",
    "    pickup_dropoff_loc = layers.HashedCrossing(name='pickup_x_dropoff', num_bins=(num_bins*num_bins)**2)([pickup_loc, dropoff_loc])\n",
    "    pickup_dropoff_embedding = layers.Embedding(name='pickup_x_dropoff_embedding', input_dim=(num_bins*num_bins)**2, output_dim=16)(pickup_dropoff_loc)\n",
    "    \n",
    "    # 4. ASSEMBLE\n",
    "    final_feature_layer = layers.concatenate([normalized_feature_layer, day_one_hot, normalized_hour_of_day, reshape_haveersine_dis, pickup_loc_embedding, dropoff_loc_embedding, pickup_dropoff_embedding], name='final_concatenate')\n",
    "    \n",
    "    # 5. BUILD THE DNN MODEL\n",
    "    h1 = layers.Dense(name='hidden_layer_1', units=h1_units, activation='relu')(final_feature_layer)\n",
    "    d1 = layers.Dropout(dropout_rate, name='dropout_1')(h1)\n",
    "    h2 = layers.Dense(name='hidden_layer_2', units=h2_units, activation='relu')(d1)\n",
    "    d2 = layers.Dropout(dropout_rate, name='dropout_2')(h2)\n",
    "    output = layers.Dense(name='output_layer', units=1, activation='linear')(d2)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mse',\n",
    "        metrics=['mse', rmse]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22180e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets\n",
    "trainds = ds_to_tf(train_ds, TRAIN_BATCH_SIZE, 'train')\n",
    "evalds = ds_to_tf(validation_ds, VALIDATION_BATCH_SIZE, 'eval')\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=build_keras_tuner_model,\n",
    "    objective=kt.Objective(\"val_rmse\", direction=\"min\"), # We want to minimize validation RMSE\n",
    "    max_trials=10,  # Total number of different hyperparameter combinations to test\n",
    "    executions_per_trial=1, # Number of times to train each combination\n",
    "    overwrite=True,\n",
    "    directory='kt_tuner_dir',\n",
    "    project_name='taxi_fare_tuning'\n",
    ")\n",
    "\n",
    "# Print a summary of the search space\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Define early stopping callback to prevent overfitting during the search\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Start the hyperparameter search\n",
    "print(\"\\n--- Starting Hyperparameter Search ---\")\n",
    "tuner.search(\n",
    "    trainds,\n",
    "    epochs=20,\n",
    "    validation_data=evalds,\n",
    "    steps_per_epoch=NUM_EVALS,\n",
    "    validation_steps=NUM_VALIDATIONS,\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "print(\"--- Hyperparameter Search Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb73983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('h1_units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}. The optimal dropout rate is {best_hps.get('dropout_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# You can also see a summary of the top results\n",
    "tuner.results_summary()\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the full dataset\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(\n",
    "    trainds, \n",
    "    epochs=50, \n",
    "    validation_data=evalds,\n",
    "    steps_per_epoch=NUM_EVALS,\n",
    "    validation_steps=NUM_VALIDATIONS,\n",
    "    callbacks=[stop_early]\n",
    ")\n",
    "\n",
    "# Find the epoch with the best validation RMSE\n",
    "val_rmse_per_epoch = history.history['val_rmse']\n",
    "best_epoch = val_rmse_per_epoch.index(min(val_rmse_per_epoch)) + 1\n",
    "print(f'Best epoch: {best_epoch}')\n",
    "print(f'Best validation RMSE: {min(val_rmse_per_epoch):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cb275",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "nr = 1\n",
    "nc=2\n",
    "\n",
    "ax = fig.add_subplot(nr,nc,1)\n",
    "ax.plot(history.history['mse'], label='mse')\n",
    "ax.plot(history.history['val_mse'], label='val_mse')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend()\n",
    "ax = fig.add_subplot(nr,nc,2)\n",
    "ax.plot(history.history['rmse'], label='rmse')\n",
    "ax.plot(history.history['val_rmse'], label='val_rmse')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_dataset_dtypes(dataset, dataset_name, num_batches_to_check=500):\n",
    "    \"\"\"\n",
    "    Iterates through a dataset to check the dtype of 'pickup_datetime'.\n",
    "    Reports the first batch where the dtype is not tf.string.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting inspection of '{dataset_name}' dataset ---\")\n",
    "    problem_found = False\n",
    "    \n",
    "    # We iterate using .as_numpy_iterator() which is easier for inspection\n",
    "    for batch_num, (features, label) in enumerate(dataset.take(num_batches_to_check).as_numpy_iterator()):\n",
    "        \n",
    "        datetime_tensor = features.get('pickup_datetime')\n",
    "        \n",
    "        if datetime_tensor is None:\n",
    "            print(f\"ERROR: Batch {batch_num} is missing the 'pickup_datetime' key!\")\n",
    "            problem_found = True\n",
    "            break\n",
    "            \n",
    "        # The core of the check: what is the dtype of the numpy array?\n",
    "        # A tf.string tensor becomes an array of bytes (dtype='O' for object).\n",
    "        # A tf.float32 tensor becomes an array of floats.\n",
    "        if datetime_tensor.dtype != 'object':\n",
    "            print(\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(f\"  PROBLEM DETECTED in '{dataset_name}' at batch number {batch_num}\")\n",
    "            print(f\"  Expected 'pickup_datetime' to be strings, but it is of type: {datetime_tensor.dtype}\")\n",
    "            print(f\"  Here are the first 5 problematic values: {datetime_tensor[:5]}\")\n",
    "            print(\"  This is the source of the 'Cast float to string' error.\")\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
    "            problem_found = True\n",
    "            break\n",
    "            \n",
    "        if (batch_num + 1) % 100 == 0:\n",
    "            print(f\"  ... inspected {batch_num + 1} batches of '{dataset_name}', all are tf.string so far.\")\n",
    "            \n",
    "    if not problem_found:\n",
    "        print(f\"--- SUCCESS: All {num_batches_to_check} inspected batches in '{dataset_name}' have the correct string dtype for 'pickup_datetime'. ---\")\n",
    "    else:\n",
    "        print(f\"--- Inspection of '{dataset_name}' finished. A data type mismatch was found. ---\")\n",
    "\n",
    "\n",
    "# --- Create temporary, non-repeating datasets for inspection ---\n",
    "\n",
    "# We only need the map that separates features and labels. No shuffle, no repeat.\n",
    "print(\"Creating inspection datasets (this may take a moment)...\")\n",
    "inspect_train_ds = tf.data.experimental.make_csv_dataset(train_ds, batch_size=32, column_names=csv_cols, column_defaults=defaults, num_epochs=1, shuffle=False).map(features_and_label)\n",
    "inspect_eval_ds = tf.data.experimental.make_csv_dataset(validation_ds, batch_size=32, column_names=csv_cols, column_defaults=defaults, num_epochs=1, shuffle=False).map(features_and_label)\n",
    "\n",
    "# --- Run the inspection ---\n",
    "inspect_dataset_dtypes(inspect_train_ds, \"training (trainds)\")\n",
    "inspect_dataset_dtypes(inspect_eval_ds, \"evaluation (evalds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958523dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# --- 1. Evaluate baseline performance ---\n",
    "print(\"Calculating baseline performance on validation set...\")\n",
    "tf.keras.utils.plot_model(best_model, 'dnn_model.png', show_dtype=True, show_shapes=True, rankdir='LR', show_layer_names=True)\n",
    "# Use your ds_to_tf function in 'eval' mode\n",
    "validation_data_tf = ds_to_tf(validation_ds, batch_size=VALIDATION_BATCH_SIZE, mode='eval')\n",
    "baseline_loss, _, baseline_rmse = best_model.evaluate(validation_data_tf, steps=NUM_VALIDATIONS, verbose=0)\n",
    "print(f\"Baseline Validation RMSE: {baseline_rmse:.4f}\\n\")\n",
    "# --- 2. Calculate Permutation Importance for each feature ---\n",
    "# We need the validation data as a pandas DataFrame to easily shuffle columns\n",
    "df_valid_full = read_data(validation_ds)\n",
    "labels_valid = df_valid_full.pop('fare_amount')\n",
    "features_valid = df_valid_full\n",
    "all_features = num_cols + string_cols\n",
    "importance_scores = {}\n",
    "print(\"Calculating permutation importance...\")\n",
    "for feature in all_features:\n",
    "# Create a copy to avoid modifying the original validation data\n",
    "    df_temp = features_valid.copy()\n",
    "    # Shuffle the values of the current feature\n",
    "    # .values ensures we shuffle the array, then we put it back\n",
    "    shuffled_values = df_temp[feature].values\n",
    "    np.random.shuffle(shuffled_values)\n",
    "    df_temp[feature] = shuffled_values\n",
    "    # Convert the modified pandas DataFrame back to a tf.data.Dataset\n",
    "    # Note: No shuffle or repeat needed for evaluation\n",
    "    shuffled_ds = tf.data.Dataset.from_tensor_slices((dict(df_temp), labels_valid))\n",
    "    shuffled_ds = shuffled_ds.batch(VALIDATION_BATCH_SIZE)\n",
    "\n",
    "    # Evaluate the model on the shuffled data\n",
    "    shuffled_loss, _, shuffled_rmse = best_model.evaluate(shuffled_ds, verbose=0)\n",
    "\n",
    "    # The \"importance\" is the increase in error\n",
    "    importance_scores[feature] = shuffled_rmse - baseline_rmse\n",
    "print(f\"  Feature: {feature:<20} | Shuffled RMSE: {shuffled_rmse:.4f} | Importance (RMSE increase): {importance_scores[feature]:.4f}\")\n",
    "# --- 3. Plot the results ---\n",
    "sorted_features = sorted(importance_scores.items(), key=lambda item: item[1])\n",
    "feature_names = [item[0] for item in sorted_features]\n",
    "scores = [item[1] for item in sorted_features]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_names, scores)\n",
    "plt.xlabel(\"Increase in RMSE (Higher is More Important)\")\n",
    "plt.title(\"Permutation Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf97869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
